{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Part X - Evaluation of Machine Learning Model Performance\n",
    "\n",
    "This notebook will present how to evaluate a machine learning model's prediction performance. We will do so by computing R-squared and mean squared error for regression models; and accuracy, precision and recall for classification models. The notebook focuses mostly on the performance measures; data preprocessing is done to some extend, however is not extensive and the final performance of the machine learning models might not be the best that can be achieved. You are free to play with the datasets on your own to try to improve the prediction performance of already used models, or use different ones.\n",
    "\n",
    "## Importing libraries\n",
    "Below you can find all libraries that will be used in this notebook. You should run it everytime you open the notebook, otherwise some functions might not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression\n",
    "\n",
    "This section presents how regression model can be evaluated using R-squared and Mean Squared Error. We will use wine quality dataset for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(dataset_url, sep=';')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to predict quality of the wine based on other characteristics, therefore we will pop the column from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = data.pop('quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will normalise the feature vector, column by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    vector = data[column].values.reshape(1, len(data[column]))\n",
    "    normalized_vector = preprocessing.normalize(vector, norm=\"l2\")\n",
    "    data[column] = normalized_vector.reshape(len(data[column]), 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train, cross-validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into (train + cross validation) and test sets\n",
    "X_x, X_test, y_x, y_test = train_test_split(data, target, test_size=0.1, random_state=42)\n",
    "\n",
    "# split into train and cross validation\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_x, y_x, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_cv.shape, y_cv.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will gradually include column by column in our prediction model. We will start with *fixed acidicity*, then we will add *volatile acidity*, etc.\n",
    "\n",
    "First, we will inspect the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a list of \"no_features\" where we will specify the indexes of columns that will be included in the regresson model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_features = [i for i in range(1, 12)]\n",
    "no_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create four arrays in which we will store the performance of the model during each iteration: *r2_train*, *r2_cv*, *mean_2_error_train*, and *mean_2_error_cv*. In each iteration we will slice the original dataset to include only specific columns by using *new_x_train = X_train[data_columns[:i]]*. We will train our model on *new_x_train* variable instead of *X_train*.\n",
    "\n",
    "After obtaining prediction values for the train and cross-valisation datasets, we will round the values using ** *round()* ** function. Rounding will be performed because wine quality feature is an integer, and linear regresion provides prediction values as floats e.g. 5.6.\n",
    "\n",
    "In each iteration we will compute the R-square and Mean Squared Error by executing functions ** *r2_score( true_value, predicted_value )* ** and ** *mean_squared_error( true_value, predicted_value )* **for both training and cross-validation sets.\n",
    "\n",
    "Both metrics will be appended to *r2_train*, *r2_cv*, *mean_2_error_train*, and *mean_2_error_cv* lists and plotted to observe how the error and R-squared improves while including more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2_train = []\n",
    "r2_cv = []\n",
    "mean_2_error_train = []\n",
    "mean_2_error_cv = []\n",
    "\n",
    "data_columns = X_train.columns\n",
    "\n",
    "for i in no_features:\n",
    "    new_x_train = X_train[data_columns[:i]]\n",
    "    new_x_cv = X_cv[data_columns[:i]]\n",
    "    \n",
    "    regression = linear_model.LinearRegression()\n",
    "    regression.fit(new_x_train, y_train)\n",
    "    \n",
    "    y_train_pred = regression.predict(new_x_train)\n",
    "    y_cv_pred = regression.predict(new_x_cv)\n",
    "    \n",
    "    y_train_pred = y_train_pred.round()\n",
    "    y_cv_pred = y_cv_pred.round()\n",
    "    \n",
    "    r2_score_train = r2_score(y_train, y_train_pred)\n",
    "    r2_score_cv = r2_score(y_cv, y_cv_pred)\n",
    "    \n",
    "    m2e_train = mean_squared_error(y_train, y_train_pred)\n",
    "    m2e_cv = mean_squared_error(y_cv, y_cv_pred)\n",
    "    \n",
    "    r2_train.append(r2_score_train)\n",
    "    r2_cv.append(r2_score_cv)\n",
    "    mean_2_error_train.append(m2e_train)\n",
    "    mean_2_error_cv.append(m2e_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot our R-square and Mean Squared Erorr using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure() \n",
    "plt.plot(range(0, len(r2_train)), r2_train, range(0, len(r2_cv)), r2_cv)\n",
    "plt.title(\"Linear Regression R^2\")\n",
    "plt.legend(['TRAIN', 'CROSS-VALIDATION'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(mean_2_error_train)), mean_2_error_train, range(0, len(mean_2_error_cv)), mean_2_error_cv)\n",
    "plt.title(\"Linear Regression Mean Squared Error\")\n",
    "plt.legend(['TRAIN', 'CROSS-VALIDATION'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Can you try to describe what happens with the R-squared and Mean Squared Errors while we add more features? Are they increasing or decreasing? Is it desirable behaviour?\n",
    " \n",
    " After choosing number of features and setting parameters, we can evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression = linear_model.LinearRegression()\n",
    "regression.fit(X_train, y_train)\n",
    "    \n",
    "y_test_pred = regression.predict(X_test)\n",
    "y_test_pred = y_test_pred.round()\n",
    "\n",
    "print(\"R^2:\", r2_score(y_test, y_test_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which yields similar performance. You are free to try other machine learning models to check if they can improve prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "This section will show how to evaluate performance of the classification model using the most common metrics: accuracy, prediction, and recall.\n",
    "\n",
    "We will work with the Traffic Accident dataset, downloaded from Leeds City Council website. If you are interested in more information about the dataset you can visit: https://data.gov.uk/dataset/road-traffic-accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = \"./datasets/Leeds_RTC2.csv\"\n",
    "data = pd.read_csv(file_name, sep=',')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this dataset for *Casualty Severity* prediction.\n",
    "\n",
    "Majority of the features are categorical features, therefore we will perform numerical encoding on them  (However for some cases one hot encoding might be more appropriate - we will not cover this issue in this notebook). We will remove *Reference Number* since it is not relevant for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['Reference Number']\n",
    "\n",
    "data['1st Road Class'] = data['1st Road Class'].astype('category')\n",
    "data['1st Road Class'] = data['1st Road Class'].cat.codes\n",
    "\n",
    "data['Road Surface'] = data['Road Surface'].astype('category')\n",
    "data['Road Surface'] = data['Road Surface'].cat.codes\n",
    "\n",
    "data['Lighting Conditions'] = data['Lighting Conditions'].astype('category')\n",
    "data['Lighting Conditions'] = data['Lighting Conditions'].cat.codes\n",
    "\n",
    "data['Weather Conditions'] = data['Weather Conditions'].astype('category')\n",
    "data['Weather Conditions'] = data['Weather Conditions'].cat.codes\n",
    "\n",
    "data['Casualty Class'] = data['Casualty Class'].astype('category')\n",
    "data['Casualty Class'] = data['Casualty Class'].cat.codes\n",
    "\n",
    "data['Casualty Severity'] = data['Casualty Severity'].astype('category')\n",
    "data['Casualty Severity'] = data['Casualty Severity'].cat.codes\n",
    "\n",
    "data['Sex of Casualty'] = data['Sex of Casualty'].astype('category')\n",
    "data['Sex of Casualty'] = data['Sex of Casualty'].cat.codes\n",
    "\n",
    "data['Type of Vehicle'] = data['Type of Vehicle'].astype('category')\n",
    "data['Type of Vehicle'] = data['Type of Vehicle'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = data.pop('Casualty Severity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find the code to perform column normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    vector = data[column].values.reshape(1, len(data[column]))\n",
    "    normalized_vector = preprocessing.normalize(vector, norm=\"l2\")\n",
    "    data[column] = normalized_vector.reshape(len(data[column]), 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train, cross-validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into (train + cross validation) and test sets\n",
    "X_x, X_test, y_x, y_test = train_test_split(data, target, test_size=0.1, random_state=42)\n",
    "\n",
    "# split into train and cross validation\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_x, y_x, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_cv.shape, y_cv.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly as for the regression model, we will gradually include more columns in the model and observe how accuracy, prediction and recall changes. We will use ** *accuracy_score( true_value, predicted_value )* ** for calculating accuracy, ** *recall_score( true_value, predicted_value, average )* ** for calculating recall, and ** *precision_score( true_value, predicted_value, average )* ** for calculating precision. Average in precision and recall defines the strategy to calculate metrics where there are more than 2 categories. For more information use: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "\n",
    "Similarly as for the regression case, we will round the prediction result, as we cannot predict 5.5 category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "accuracy_train = []\n",
    "accuracy_cv = []\n",
    "precision_train = []\n",
    "precision_cv = []\n",
    "recall_train = []\n",
    "recall_cv = []\n",
    "\n",
    "no_features = [i for i in range(1, 12)]\n",
    "\n",
    "data_columns = X_train.columns\n",
    "\n",
    "for i in no_features:\n",
    "    new_x_train = X_train[data_columns[:i]]\n",
    "    new_x_cv = X_cv[data_columns[:i]]\n",
    "    \n",
    "    logistic_reg = linear_model.LogisticRegression()\n",
    "    logistic_reg.fit(new_x_train, y_train)\n",
    "    \n",
    "    y_train_pred = logistic_reg.predict(new_x_train)\n",
    "    y_cv_pred = logistic_reg.predict(new_x_cv)\n",
    "    \n",
    "    y_train_pred = y_train_pred.round()\n",
    "    y_cv_pred = y_cv_pred.round()\n",
    "    \n",
    "    acc_train = accuracy_score(y_train, y_train_pred)\n",
    "    acc_cv = accuracy_score(y_cv, y_cv_pred)\n",
    "    prec_train = recall_score(y_train, y_train_pred, average='macro')\n",
    "    prec_cv = recall_score(y_cv, y_cv_pred, average='macro')\n",
    "    rec_train = precision_score(y_train, y_train_pred, average='macro')\n",
    "    rec_cv = precision_score(y_cv, y_cv_pred, average='macro')\n",
    "    \n",
    "    accuracy_train.append(acc_train)\n",
    "    accuracy_cv.append(acc_cv)\n",
    "    precision_train.append(prec_train)\n",
    "    precision_cv.append(prec_cv)\n",
    "    recall_train.append(rec_train)\n",
    "    recall_cv.append(rec_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that a warning occurred: UndefinedMetricWarning. It appears here because Logistic Regression did not predict any occurence of some label. This might happen, especially in highly unbalanced datasets.\n",
    "\n",
    "Plotting accuracy, precition, and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure() \n",
    "plt.plot(range(0, len(accuracy_train)), accuracy_train, range(0, len(accuracy_cv)), accuracy_cv)\n",
    "plt.title(\"Logistic Regression Accuracy\")\n",
    "plt.legend(['TRAIN', 'CROSS-VALIDATION'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(precision_train)), precision_train, range(0, len(precision_cv)), precision_cv)\n",
    "plt.title(\"Logistic Regression Precision\")\n",
    "plt.legend(['TRAIN', 'CROSS-VALIDATION'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(recall_train)), recall_train, range(0, len(recall_cv)), recall_cv)\n",
    "plt.title(\"Logistic Regression Recall\")\n",
    "plt.legend(['TRAIN', 'CROSS-VALIDATION'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you comment what is happening on the plots? Why they are not improving with including more features?\n",
    "\n",
    "There are more metrics except precision and recall, for example F1 score. You can try to write the code that will compute and plot performance of the model using this metric, for more information refer to: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "And finally testing our model on test data. Can you comment on the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression = linear_model.LogisticRegression()\n",
    "regression.fit(X_train, y_train)\n",
    "    \n",
    "y_test_pred = regression.predict(X_test)\n",
    "y_test_pred = y_test_pred.round()\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
