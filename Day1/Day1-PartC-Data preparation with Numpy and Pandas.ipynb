{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - Part C: Data preparation with Numpy and Pandas\n",
    "\n",
    "Numpy and Pandas are python libraries which automate most of the data manipulation tasks for you. This notebook will provide an overview of these and guide you though data preparation steps including: cleaning, encoding and normalization.\n",
    "\n",
    "Below is the code importing all the libraries that will be used throughout this notebook. Remember to run this line everytime you open the notebook. Otherwise, majority of the functions used here will not work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Numpy\n",
    "Numpy adds support for large, multidimensional array manipulation. It supports operations such as addition, multiplication, inversion and many other. Here, you will learn how to create numpy arrays and matrices, and how to perform basic operations on them.\n",
    "\n",
    "For more information refer to: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\n",
    "\n",
    "## 1.1. Basic numpy operations\n",
    "In numpy, you can create arrays from lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some_array = np.array([1, 2, 3, 4, 5])\n",
    "print(some_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single array can contain different objects: strings, floats, integers etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some_array = np.array(['blue', 'red', 'orange', 'white'])\n",
    "print(some_array)\n",
    "\n",
    "some_array = np.array([1.5, 2, 3.6, 4.1, 5.17])\n",
    "print(some_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access elements of an array using [ ], similarly as in python's lists. Indexes of elements start from 0, therefore the code below accesses the ** *2nd* ** element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some_array[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays are mutable, therefore its elements can be changed by assigning a value. Below the ** *4th* ** element of an array has been changed to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Before change:\")\n",
    "print(some_array)\n",
    "\n",
    "some_array[3] = 15\n",
    "\n",
    "print(\"\\nAfter change:\")\n",
    "print(some_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can assign the whole numpy array to a variable, however be careful when you are doing it as modifying one array will modify the other array as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some_array = np.array([1.5, 2, 3.6, 4.1, 5.17])\n",
    "some_array2 = some_array\n",
    "\n",
    "print(\"Before change:\")\n",
    "print(\"some_array:\", some_array)\n",
    "print(\"some_array2:\", some_array2)\n",
    "\n",
    "some_array[3] = -6\n",
    "\n",
    "print(\"\\nAfter change:\")\n",
    "print(\"some_array:\", some_array)\n",
    "print(\"some_array2:\", some_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, elements in **both** arrays were modified even if only one of them was changed! It is because variables *some_array* and *some_array2* are pointers to arrays. We will not discuss pointers here, and you are not required to know how they work, however keep in mind that assigning numpy arrays to variables will \"bound\" them together and they will be susceptible to changes made on the other copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 2D arrays\n",
    "you can create arrays which are 2-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array( [ [1, 2, 3], [3, 4, 5] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a 2D array, you need to pass a list of columns to ** *np.array()* ** function, where each column is a list of row elements. You are provided with come examples below, but feel free to experiment with sizes!\n",
    "\n",
    "Matrix 3x2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array( [ [1, 2], [3, 3], [4, 5] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix 1x6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array( [ [1, 2, 3, 3, 4, 5] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix 6x1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array( [ [1], [2], [3], [3], [4], [5] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Basic operations on arrays\n",
    "\n",
    "Numpy is very efficient with array operations. Given two arrays A and B you can add them, multiply them, or calculate the inverse. Examples are provided to you below. For more information refer to: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([ [1, 2], [0, 4]])\n",
    "B = np.array([ [4, 0], [3, -1]])\n",
    "\n",
    "print(A)\n",
    "print()\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A - B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate inverse you have to import ** *inv* ** package. This is done for you in the first script at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Functions for creating custom arrays\n",
    "Numpy provides functions that enable to generate arrays for you. Examples are provided below.\n",
    "\n",
    "** *np.zeros( (2, 2) )* ** creates 2x2 matrix containing only zeros, ** *np.ones( (2, 3) )* ** creates 2x3 matrix containing only ones, and ** *np.random.rand(3, 2)* ** creates 3x2 matrix containing random numbers between 0 - 1.\n",
    "\n",
    "You can create 1-dimensional arrays, 2- or even more dimensional matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.zeros((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.ones((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.rand(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4D matrix 3x2x2x4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.rand(3, 2, 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Slicing arrays\n",
    "Numpy allows to slice the original array and work on the slide instead of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some_array = np.random.rand(3, 3)\n",
    "\n",
    "print(some_array)\n",
    "\n",
    "slice_array = some_array[0:2, 1:3]\n",
    "\n",
    "print()\n",
    "print(slice_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify the range which columns/rows should be included by using the format *array[ column_start:column_end, row_start:row_end ]*. Rows and columns in numpy (and also in Pandas) start indexing from 0, therefore if you want to include first and second column you need to use 0:2, if second and third use 1:3 etc. A notation 1:4 means including elements from index 1, 2 and 3. Note, that 4 is not included.\n",
    "\n",
    "Again, remember that working on the slice will modify the original array too! The slice of an array has different indexing now, therefore the same element in the example below will have [0, 0] location in the slice array, and [0, 1] location in the original array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Before change:\")\n",
    "print(\"some_array:\\n\", some_array)\n",
    "print(\"some_array2:\\n\", slice_array)\n",
    "\n",
    "slice_array[0, 0] = -6\n",
    "\n",
    "print(\"\\nAfter change:\")\n",
    "print(\"some_array:\\n\", some_array)\n",
    "print(\"some_array2:\\n\", slice_array)\n",
    "\n",
    "print()\n",
    "print(slice_array[0, 0], some_array[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction to Pandas\n",
    "\n",
    "Pandas, similarly to numpy, provides support with large, multidimensional arrays. Additionally it is equipped with many more functions. The following section provides some key information about the use of the library for data manipulation. You are free to read more here: http://pandas.pydata.org/pandas-docs/stable/tutorials.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Importing data from files\n",
    "The ** *read_csv()* ** function enables to read CSV files, however pandas enables to read other formats e.g. excel files. Below, is an exemplary code snippet which reads the file *data.csv* and loads it into pandas DataFrame.\n",
    "\n",
    "Useful arguments of the function:\n",
    "- ** *sep* ** specifies the separator with which cells are separated with, \n",
    "- *** encoding* ** enables to change the file encoding. The default encoding is *utf-8*, however you might try *ISO-8859-1* if utf-8 does not work.\n",
    "\n",
    "Run the code below to examine first rows of the dataset with ** *dataFrame.head()* ** function. Try to change the encoding to *utf-8* and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv('./datasets/ecommerce_data/e_commerce.csv', sep=',', encoding='ISO-8859-1')\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Creating DataFrame from a python dictionary\n",
    "Alternatively, you can create Pandas DataFrame from a dictionay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [{'color_name': 'black', 'R': 0, 'G': 0, 'B': 0}, {'color_name': 'white', 'R': 255, 'G': 255, 'B': 255}, \n",
    "        {'color_name': 'red', 'R': 255, 'G': 0, 'B': 0}, {'color_name': 'blue', 'R': 0, 'G': 0, 'B': 255},\n",
    "        {'color_name': 'green', 'R': 0, 'G': 255, 'B': 0}]\n",
    "\n",
    "dataFrame = pd.DataFrame(data)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Inspecting the dataset\n",
    "\n",
    "To inspect the dataset use ** *head()* ** function. This will show you first 5 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoever, it will work only when ** *head()* ** is executed at the end of the code block. If you want to print your dataframe somewhere in the middle of the code block use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataFrame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Selecting, modifying and removing rows/columns\n",
    "\n",
    "### Selecting a column\n",
    "A good thing about Pandas is that you can refer to specific columns simply by using its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame['color_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting rows\n",
    "To select range of rows use the code below. DataFrame[2:4] selects all columns for rows with indexes 2 and 3 (4 is not included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to select specific rows use ** *loc* **, or ** *iloc* **. loc will return the row that matches given label, whereas iloc will return a row that has ith position in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.loc[[0,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select first e.g. 2 rows of the dataset, like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or last rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting multiple columns\n",
    "You can select multiple columns by passing it as a list of column names, just like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_slice = dataFrame[['B','color_name']]\n",
    "column_slice.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While slicing this way, the slice created is not a pointer to the original data frame, therefore modifying it will not change the original dataset. However, it is not the case with .iloc and .loc attributes because they **WILL** return a pointer, therefore if you change the slice obtained using .iloc or .loc it will change your original data frame as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_slice = dataFrame[['B','color_name']]\n",
    "\n",
    "print(\"\\nBefore changes:\")\n",
    "print(column_slice)\n",
    "print()\n",
    "print(dataFrame)\n",
    "\n",
    "column_slice.loc[0, 'B'] = 14\n",
    "\n",
    "print(\"\\nAfter changes:\")\n",
    "print(column_slice)\n",
    "print()\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more on indexing here: http://pandas.pydata.org/pandas-docs/stable/indexing.html; and more on slice modification here: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names and indices\n",
    "You can return column names by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the row numbers/names by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional selection\n",
    "You can apply conditional selection by using ** dataFrame[*column_name*] *condition* *value* ** as specified below. The result of such expression is a True/False data frame which can be used to display rows which fulfil the condition. For example, the code below selects all rows that have *R* higher than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "select_greater_than_100 = (dataFrame['R'] > 100)\n",
    "select_greater_than_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame[select_greater_than_100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new columns\n",
    "You can create a new column just by assigning values to non-existent column name. You can use completely new values, or create column using values from another one, as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame['color_number'] = 0\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame['color_number'] = 65536 * dataFrame['R'] + 256 * dataFrame['G'] + dataFrame['B']\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing columns\n",
    "You can remove the column by using ** *del* **:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del dataFrame['color_number']\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simply pop it from the dataset. This operation will remove the column and return its values creating new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dataFrame = dataFrame.pop('color_name')\n",
    "print(new_dataFrame)\n",
    "print()\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now adding it back to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame['color_name'] = new_dataFrame\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Data Science Process\n",
    "\n",
    "The usual data science process contains the following steps:\n",
    "1. Initial inspection of the dataset\n",
    "2. Data preparation (cleaning, feature selection, normalization and splitting dataset into train/cross-validation/test)\n",
    "4. Building the model\n",
    "5. Evaluating the model performance\n",
    "\n",
    "Before the machine learning can be applied to the dataset, a significant effort needs to be put into preparing the dataset for the model. Poorly performed data preparation might result in poor performance or unreliable results. This jupyter notebook will cover initial inspection of the dataset, data cleaning, normalisation and splitting dataset into train/cross-validation/test.\n",
    "\n",
    "We will do so on the exemplary e-commerce dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data cleaning and initial inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv('./datasets/ecommerce_data/e_commerce.csv', sep=',', encoding='ISO-8859-1', dtype={\n",
    "    'InvoiceNo': str, \n",
    "    'StockCode': str,\n",
    "    'Description': str,\n",
    "    'Quantity': np.float64,\n",
    "    'InvoiceDate': str,\n",
    "    'UnitPrice': np.float64,\n",
    "    'CustomerID': np.float64,\n",
    "    'Country': str\n",
    "})\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset includes 8 columns: *Invoice No.*, *StockCode*, *Description*, *Quantity*, *InvoiceDate*, *UnitPrice*, *CustomerID*, and *Country*.\n",
    "\n",
    "Numerical variables include: *Quantity*, *UnitPrice*, and *CustomerID*.\n",
    "\n",
    "Categorical variables include: *InvoiceNo*, *StockCode*, *Description*, and *Country*, as for now they are stored int the data frame as strings\n",
    "\n",
    "There is only one datetime type variable: *InvoiceDate*, again stored in the data frame as a string.\n",
    "\n",
    "You can inspect the types of the variables using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Replacing missing values\n",
    "We will check if there are any variables that are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are *Description* and *CustomerID* features which are empty. The decision what to do with these depends on the circumstances. The most popular options are:\n",
    "\n",
    "- remove missing values\n",
    "- replace them with mean/most common category\n",
    "- replace with the value of the previous row (forward-fill)\n",
    "- replace with the value of the next row (backward-fill)\n",
    "\n",
    "#### Removing missing values\n",
    "Removing the missing values is done using ** *dropna()* ** function. It is important to assign reduced DataFrame to a variable, otherwise changes will not have any effect. Example of such behaviour is presented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataFrame.shape)\n",
    "\n",
    "dataFrame.dropna()\n",
    "\n",
    "print(dataFrame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the size of the DataFrame did not change, which means that the empty rows were **NOT** removed. Below is the corrected version of the code. The dataset has been reduced by almost 67 000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataFrame.shape)\n",
    "\n",
    "dataFrame = dataFrame.dropna()\n",
    "\n",
    "print(dataFrame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing missing values with the mean\n",
    "First we will set *Quantity* in the row with index 1 to null, and then observe how it has been replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.loc[1, 'Quantity'] = np.nan\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing missing values in column *Quantity* with mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame['Quantity'] = (dataFrame['Quantity'].replace(np.nan, dataFrame['Quantity'].mean()))\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing missing values with previous row (forward-fill)\n",
    "\n",
    "First, setting null values in *StockCode* and *InvoiceDate* in the row with index 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.loc[7, 'StockCode'] = np.nan\n",
    "dataFrame.loc[7, 'InvoiceDate'] = np.nan\n",
    "dataFrame[3:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing missing values with the previous value (forward fill). Please note that this method will replace missing values in all columns, with values respective to that column; and might find very useful when replacing missing values in time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame = dataFrame.fillna(method='ffill')\n",
    "dataFrame[3:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing missing values with next row (backward-fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.loc[7, 'StockCode'] = np.nan\n",
    "dataFrame.loc[7, 'InvoiceDate'] = np.nan\n",
    "dataFrame[3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame = dataFrame.fillna(method='bfill')\n",
    "dataFrame[3:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Initial inspection and data cleaning\n",
    "\n",
    "Now we can investigate how the numerical dataset looks like by using ** *describe()* ** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the *Quantity* feature can have negative values, which is not what we would expect. These values can be removed from the dataset by applying conditional selection discussed before.\n",
    "\n",
    "First, we see how many of these have non-positive *Quantity* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dataFrame[dataFrame['Quantity'] <= 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be selecting these rows using conditional selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataFrame.shape)\n",
    "\n",
    "indexes_with_negative_height = dataFrame[dataFrame['Quantity'] < 0].index\n",
    "dataFrame = dataFrame.drop(indexes_with_negative_height)\n",
    "\n",
    "print(dataFrame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the dataset has been reduces exactly by 4098 rows.\n",
    "\n",
    "Interestingly, there are 13 rows which have unit price equal to 0. This raises a warning flag since it is not often for an item to be for free. We will be displaying these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dataFrame[dataFrame['UnitPrice'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame[dataFrame['UnitPrice'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However unusual, these items might still be valid entries because they might have been sold as an addition to other item, or bought with a voucher, therefore they will be kept in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can further inspect the dataset by calling  functions e.g. min(), max(), mean(), etc. explicitly on the dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the strings the minimum and maximum values are evaluated based on the characters included in the string. For example, a letter \"A\" will be considered as \"smaller\" than letter \"Z\". Moreoever, string digits e.g. \"8\" are always considered to be smaller than letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can apply functions ** *mode()* ** or ** *mean()* **.\n",
    "\n",
    "More functions can be found here: https://pandas.pydata.org/pandas-docs/stable/api.html#api-dataframe-stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the correlation coefficients between columns of numerical values by using *corr()* function. In the next notebook on data visualisation you will learn how to create a correlation heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalid categorical values\n",
    "\n",
    "If you want to inspect if the categorical column contains invalid entries you can use *value_counts()* function, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies for numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the values seem to be valid entries, however some of the countries are categorized as \"Unspecified\". It is our decision whether we want to remove rows which have this value or not, and in this case we will do this be executing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataFrame.shape)\n",
    "\n",
    "dataFrame = dataFrame.drop(dataFrame[dataFrame['Country'] == 'Unspecified'].index)\n",
    "\n",
    "print(dataFrame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where *dataFrame[dataFrame['Country'] == 'Unspecified']* selects all the rows that have 'Unspecified' country; ** *.index* ** returns indexes associated with these rows; and ** *dataFrame.drop()* ** removes these indexes from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed exactly 72 rows, as specified for 'Unspecified' by value_counts()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Encoding categorical variables\n",
    "\n",
    "Most machine learning methods do not directly work with categories, there is a need to transform these into numerical values. There are two options to do it:\n",
    "\n",
    "- encoding each category by number e.g. Monday - 1, Tuesday - 2 etc.\n",
    "- creating dummy columns which encode the category. For example for day of the week, we would create 7 columns, each corresponsing to one day of the week.\n",
    "\n",
    "To demonstrate both approaches, we will switch to the color dataset used at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [{'color_name': 'black', 'R': 0, 'G': 0, 'B': 0}, {'color_name': 'white', 'R': 255, 'G': 255, 'B': 255}, \n",
    "        {'color_name': 'red', 'R': 255, 'G': 0, 'B': 0}, {'color_name': 'blue', 'R': 0, 'G': 0, 'B': 255},\n",
    "        {'color_name': 'green', 'R': 0, 'G': 255, 'B': 0}, {'color_name': 'green', 'R': 0, 'G': 255, 'B': 0}]\n",
    "\n",
    "new_dataFrame = pd.DataFrame(data)\n",
    "new_dataFrame['color_name2'] = new_dataFrame['color_name']\n",
    "new_dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Numerical encoding of categorical variables\n",
    "\n",
    "First, will transform the type of the column to *category*, so that pandas map each category with the unique code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dataFrame['color_name'] = new_dataFrame['color_name'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will update the column with corresponding category codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dataFrame['color_name'] = new_dataFrame['color_name'].cat.codes\n",
    "new_dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each color has now each category assigned: *black* - 0, *white* - 4, *red* - 3, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Creating dummy columns (also called as One Hot Encoding)\n",
    "\n",
    "Execute ** *pd.get_dummies()* ** function to expand a specific column to dummy columns. *columns* parameter specifies the names of columns that will be expanded using one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dataFrame = pd.get_dummies(new_dataFrame, columns=['color_name2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the *color_name2* column has been expanded to 5 binary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Unbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Datasets often might contain unbalanced categorical variables; that means that some of these values appear more frequenty than the other. When the unbalanced feature is the target i.e. the feature that we want to predict using machine learning models, it is a problem because our model will be trained to predict the majority category for most of the time. More on this will be presented in the notebook dealing with evaluating performance of the machine learning model.\n",
    "\n",
    "There are many different methods to deal with the problem of unbalanced datasets, however in this notebook we will focus on two of them: under-sampling and over-sampling. Under-sampling reduce the size of our original dataset removing instances of the majority class to match the minority class. Over-sampling adds new data points which are similar to these of the minority class. \n",
    "\n",
    "Please note, that under-sampling and over-sampling might not be the best to deal with unbalanced datasets, as it will introduce bias. For example, under-sampling will remove instances of the majority class that might be meaningful. However, it still might work in practice for some cases.\n",
    "\n",
    "Under-sampling and over-sampling will be presented below, however before we do it, we will slighlty modify the e-commerce dataset.\n",
    "\n",
    "Let's assume that we are interested only in transactions which happened in United Kingdom and France. Below is the code which will select all the rows which have 'United Kingdom' value in column *Country*, and these with 'France' value in the same column.\n",
    "\n",
    "Both selections are then concatenated togehter using ** *pd.concat()* ** pandas function. Concatenate adds one data frame below the other; this implies that both data frames need to have the same columns. More about concatenation (and other merging functions) can be found here: https://pandas.pydata.org/pandas-docs/stable/merging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UKData = dataFrame.loc[dataFrame['Country'] == 'United Kingdom']\n",
    "FranceData = dataFrame.loc[dataFrame['Country'] == 'France']\n",
    "\n",
    "dataFrame = pd.concat([UKData, FranceData])\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can confirm that our dataset contain only entries related to United Kingdom and France. Moreover, we can observe that there are many more entries related to UK than to France. This means that this dataset is *unbalanced*, therefore we will be applying under-sampling and over-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Under-sampling\n",
    "\n",
    "Under-sampling is removing the instances of the majority class so that it matches the number of minority class instances. We will do it using sklearn library ** *resample* **. First, we will create slices of the original dataset, where the first slice contains the majority class (\"United Kingdom\"), and the second slice contains the minorty class (\"France\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_majority = dataFrame[dataFrame['Country']=='United Kingdom']\n",
    "df_minority = dataFrame[dataFrame['Country']=='France']\n",
    "minority_len = (len(df_minority))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will run the resample method with parameter *replace* equal to False, and n_samples equal to the number of instances in the minority class. The reduced majority class will be concatenated with the minority class, and we can observe that there is now exactly the same number of instances for majority and minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_majority_downsampled = resample(df_majority,replace=False,n_samples=minority_len,random_state=123)\n",
    "ddown = pd.concat([df_minority, df_majority_downsampled])\n",
    "ddown['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Over-sampling\n",
    "\n",
    "Over-sampling will create copies of dataset instances to match required number of samples. Below, we count number of instances in the majority class, and create additional instances in the minority class to match the majority class.\n",
    "\n",
    "We can now see that there is the same number of instances for both classes, and it is equal to the initial size of the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "majority_len = (len(df_majority))\n",
    "\n",
    "df_minority_upsampled = resample(df_minority,replace=True,n_samples=majority_len,random_state=123)\n",
    "dup = pd.concat([df_majority, df_minority_upsampled])\n",
    "dup['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Normalization\n",
    "\n",
    "Most of the machine learning algorithms require the dataset they are trained on to be normalized. Below is the example of normalization of the e-commerce dataset.\n",
    "\n",
    "First, we will be encoding categorical variables. We will use category codes for *InvoiceNo*, *StockCode* and *Description*, and One Hot Encoding for *Contry*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame['StockCode'] = dataFrame['StockCode'].astype('category')\n",
    "dataFrame['StockCode'] = dataFrame['StockCode'].cat.codes\n",
    "\n",
    "dataFrame['Description'] = dataFrame['Description'].astype('category')\n",
    "dataFrame['Description'] = dataFrame['Description'].cat.codes\n",
    "\n",
    "dataFrame['InvoiceNo'] = dataFrame['InvoiceNo'].astype('category')\n",
    "dataFrame['InvoiceNo'] = dataFrame['InvoiceNo'].cat.codes\n",
    "\n",
    "dataFrame = pd.get_dummies(dataFrame, columns=['Country'])\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *InvoiceDate* field would need to be translated into the timestamp in order to be correcrtly normalized, however as it is not the scope of this notebook we will remove it from our data frame. If you are interested, you can read more about timestamps here: https://pandas.pydata.org/pandas-docs/stable/timeseries.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dataFrame['InvoiceDate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ** *preprocessing* ** package from sklearn to normalize our dataset. We have to normalize column by column. Otherwise the normalization will take place across different column.\n",
    "\n",
    "The preprocessing package requires to pass the data in a form of a vector, therefore we will be first reshaping pandas data frame to a vector, and then reshaping it again as a column and updating the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in dataFrame.columns:\n",
    "    vector = dataFrame[column].values.reshape(1, len(dataFrame[column]))\n",
    "    normalized_vector = preprocessing.normalize(vector, norm=\"l2\")\n",
    "    dataFrame[column] = normalized_vector.reshape(len(dataFrame[column]), 1)\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Splitting the dataset\n",
    "\n",
    "In this section we will discuss two ways of splitting the datasets:\n",
    "- Splitting inot train, cross-validation and test sets\n",
    "- K-fold cross-validation\n",
    "\n",
    "We will perform the split on the e-commerce dataset, where first we will extract the variable we want to predict - let it be *UnitPrice* in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = dataFrame.pop('UnitPrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Splitting into train, cross-validation, and test sets\n",
    "\n",
    "To split the original dataset into three sets: train, cross-validation and test we can use ** *train_test_split()* ** function twice. First split will divide our dataset into (train + cross-validation) and the test set. The second split will divide the data into train and cross-validation. If we want to divide into 60% training, 20% cross-validation, and 20% test we should use at first 0.2 test size and 0.8 train size. While dividing for the second time we have to take into account that the dataset is now reduced, therefore we have to adjust the ratios to: 0.2/0.8 = 0.25 test size and 0.6/0.8 = 0.75 train size, which will give us train and cross-validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into (train + cross validation) and test sets\n",
    "X_x, X_test, y_x, y_test = train_test_split(dataFrame, target, test_size=0.2, train_size=0.8, random_state=43)\n",
    "\n",
    "# split into train and cross validation\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_x, y_x, test_size=0.25, train_size=0.75, random_state=43)\n",
    "\n",
    "print(X_train.shape, X_cv.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 K-fold\n",
    "\n",
    "To perform K-fold you can use the class provided by scikit-learn ** *Kfold()* **. The argument ** *n_splits* ** defines how many parts we want to divide the dataset to, ** *shuffle* ** indicates whether we want to shuffle the dataset before we split it. Kfold will then return the indexes of the chosen rows, from which we can build our train and test subsets. It is advised to create new DataFrame objects while doing so, because .iloc returns a pointer to the original data frame which implies that modifying the slice will modify the original dataset too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True, random_state=43)\n",
    "\n",
    "print(\"Original dataset:\")\n",
    "print(dataFrame.shape)\n",
    "\n",
    "kfold_count = 0\n",
    "for train_indexes, test_indexes in kf.split(dataFrame):\n",
    "    train_dataset = pd.DataFrame(dataFrame.iloc[train_indexes])\n",
    "    test_dataset = pd.DataFrame(dataFrame.iloc[test_indexes])\n",
    "    train_target = pd.DataFrame(target.iloc[train_indexes])\n",
    "    test_target = pd.DataFrame(target.iloc[test_indexes])\n",
    "    \n",
    "    kfold_count += 1\n",
    "    print(\"Kfold\", str(kfold_count) + \"-iteration:\")  \n",
    "    \n",
    "    print(\"Train:\", train_dataset.shape, \"Test:\", test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read more on splitting the dataset go to: http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
